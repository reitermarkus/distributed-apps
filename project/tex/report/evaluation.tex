\chapter{\label{chap:evaluation}Evaluation}

For our evaluation, we deployed our functions on IBM Cloud in the Washington region.
For each function, we tested both the Rust as well as the JavaScript version.

\begin{table}[h]
  \centering
  \begin{tabular}{|l|l|l|l|}
    \hline
    Function          & Region     & Langauge   & Time (ms) \\ \hline
    `fetch\_prices`   & Washington & Rust       & 3627      \\ \hline
    `fetch\_prices`   & Washington & JavaScript & 2333      \\ \hline
    `forecast`        & Washington & Rust       & 6689      \\ \hline
    `forecast`        & Washington & JavaScript & 6982      \\ \hline
    `process\_result` & Washington & Rust       & 2826      \\ \hline
    `process\_result` & Washington & JavaScript & 1382      \\ \hline
    `create\_chart`   & Washington & Rust       & 1396      \\ \hline
    `create\_chart`   & Washington & JavaScript & 1301      \\ \hline
  \end{tabular}
  \caption{Benchmark Results}
  \label{tab:benchmark}
\end{table}

As can be seen in \cref{tab:benchmark}, in almost all cases the JavaScript version is faster.
In the cases where JavaScript is faster, the functions are really light-weight and
mostly only consist of a couple API calls and some basic logic in-between. We assume
that there exists a steady supply of JavaScript containers which are already
running while Rust containers have to wait for a bare container to start.
When we look at the forecast function, however, we see that the Rust version is actually faster.
This function makes quite a few API calls and additionally transforms a JSON document
into CSV. In the Rust function, this is done in-memory, while the JavaScript function
saves the CSV to a file before uploading, which might explain why the Rust version
is faster in this case.

Another remark here has to be made to clarify that the numbers in \cref{tab:benchmark}
are for cached results, meaning the forecast function was run previously
and the forecast does not have to be done from scratch. Creating a forecast from
scratch can take from 45 minutes to 1 hour and 30 minutes. This means that first of all,
the deployed function will time out in any case since the maximum runtime of a function
on IBM Cloud is 10 minutes, and secondly, the difference between forecast runs can be up
to 45 minutes, which makes benchmarking them meaningless.

\textcolor{magenta}{Tentative evaluation length: between two and three pages.}

%
%
%
\section{Testing methodology}

Describe the experiment setup you used to obtain the evaluation results (used FaaS systems, regions, function deployments, number of experiment repetitions, measured metrics, etc.).

Make sure that you provide enough details to make the experiment reproducible.



%
%
%
\section{Evaluation}

Present and discuss the results. Show some chart - makespan for each FC deployment, as well as from the scheduled FC.

Explain the insights provided by the experiments. Explain outliers, if present.
This part of the report should provide evidence that the previously formulated project objectives are achieved with your implementation of the FC.

If results are not as expected, discuss why.

Discuss some findings from your measurements:
\begin{itemize}
    \item good/bad scheduler
    \item (not)expected speedup
    \item some observations per provider, per region, per function type, per function implementation, per function deployment
\end{itemize}
