\chapter{\label{chap:evaluation}Evaluation}

For our evaluation, we deployed our functions on IBM Cloud in the Washington region.
For each function, we tested both the Rust as well as the JavaScript version.

\begin{table}[h]
  \centering
  \begin{tabular}{|l|l|l|l|}
    \hline
    Function          & Region     & Langauge   & Time (ms) \\ \hline
    `fetch\_prices`   & Washington & Rust       & 3627      \\ \hline
    `fetch\_prices`   & Washington & JavaScript & 2333      \\ \hline
    `forecast`        & Washington & Rust       & 6689      \\ \hline
    `forecast`        & Washington & JavaScript & 6982      \\ \hline
    `process\_result` & Washington & Rust       & 2826      \\ \hline
    `process\_result` & Washington & JavaScript & 1382      \\ \hline
    `create\_chart`   & Washington & Rust       & 1396      \\ \hline
    `create\_chart`   & Washington & JavaScript & 1301      \\ \hline
  \end{tabular}
  \caption{Benchmark Results}
  \label{tab:benchmark}
\end{table}

As can be seen in \cref{tab:benchmark}, in almost all cases the JavaScript version is faster.
In the cases where JavaScript is faster, the functions are really light-weight and
mostly only consist of a couple API calls and some basic logic in-between. We assume
that there exists a steady supply of JavaScript containers which are already
running while Rust containers have to wait for a bare container to start.
When we look at the forecast function, however, we see that the Rust version is actually faster.
This function makes quite a few API calls and additionally transforms a JSON document
into CSV. In the Rust function, this is done in-memory, while the JavaScript function
saves the CSV to a file before uploading, which might explain why the Rust version
is faster in this case.


\textcolor{magenta}{Tentative evaluation length: between two and three pages.}

%
%
%
\section{Testing methodology}

Describe the experiment setup you used to obtain the evaluation results (used FaaS systems, regions, function deployments, number of experiment repetitions, measured metrics, etc.).

Make sure that you provide enough details to make the experiment reproducible.



%
%
%
\section{Evaluation}

Present and discuss the results. Show some chart - makespan for each FC deployment, as well as from the scheduled FC.

Explain the insights provided by the experiments. Explain outliers, if present.
This part of the report should provide evidence that the previously formulated project objectives are achieved with your implementation of the FC.

If results are not as expected, discuss why.

Discuss some findings from your measurements:
\begin{itemize}
    \item good/bad scheduler
    \item (not)expected speedup
    \item some observations per provider, per region, per function type, per function implementation, per function deployment
\end{itemize}
